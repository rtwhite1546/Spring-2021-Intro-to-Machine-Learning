{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Machine Learning, Data Matrix, Linear Regression\n",
    "\n",
    "# Lecture 1 (Mon, Jan 11)\n",
    "\n",
    "A machine learning problem generally has three parts\n",
    "\n",
    "* A task $T$\n",
    "* A performance measure $P$\n",
    "* An experience $E$\n",
    "\n",
    "A computer program learns from experience $E$ with respect to some task $T$ and performance measure $P$ if it improves at task $T$, measured by $P$, with experience $E$. For example,\n",
    "\n",
    "* **Task**: Identify pictures as having cats or not having cats in them\n",
    "* **Performance measure**: The proportion of pictures with cats does our computer properly identify\n",
    "* **Experience**: The computer will be given 500 images with cats and 500 images without cats, each with labels of \"cat\" or \"no cat\"\n",
    "\n",
    "Machine learning tasks are generally too difficult for pre-designed programs to do. Instead, we write programs for computers to learn to solve them. For example, we might want a machine learning algorithm to determine if a picture has a cat in it:\n",
    "\n",
    "* We **WOULD NOT** give a computer set of instructions to decide how to find cats. (What would the instructions be?!)\n",
    "\n",
    "* We **WOULD** feed many cat pictures and non-cat pictures to the computer, let label them according to some instructions (usually with some randomly initialized parameters), and give it some instructions for how to tweak its labels, and \n",
    "\n",
    "There are many types of tasks. Two large classes of tasks are supervised and unsupervised learning tasks.\n",
    "\n",
    "* **Supervised learning algorithms** have an experience of observing a dataset of examples *with* labels a correct algorithm would output.\n",
    "\n",
    "* **Unsupervised learning algorithms** have an experience of observing a dataset *without* labels and seek to learn useful patterns in the dataset.\n",
    "    \n",
    "There are some other types of problems, but these groups are most common and the primary focus of this class. Below, we discuss some common tasks in each category. It is not meant to be an exhaustive list but rather a brief outline of what the tasks are, examples of each, and some common methods for attacking these tasks.\n",
    "\n",
    "It should be noted that machine learning practitioners commonly use multiple methods, build methods customized to their problems, and create pipelines using multiple methods in a specified sequence.\n",
    "\n",
    "## Supervised Learning\n",
    "\n",
    "For most supervised learning problems, we have a dataset of $n$ **examples** or **data points**, which can be presented as points in space\n",
    "\n",
    "$$ x_i=(x_{i1}, ..., x_{id})\\in\\mathbb{R}^d$$\n",
    "\n",
    "and we try to predict a function $f:\\mathbb{R}^d\\to\\mathbb{R}^m$ mapping these points $x_i$ to some **label** or **target** $y_i\\in\\mathbb{R}^m$. Each component of $x_i$ is usually called a **feature** or an **attribute**.\n",
    "\n",
    "### Classification Problems\n",
    "\n",
    "In a classification task, the learning algorithm tries to predict which of $k$ disjoint categories a datapoint belongs to--e.g. to estimate a function $f:\\mathbb{R}^d\\to\\{1, ..., k\\}$.\n",
    "\n",
    "Identifying pictures of cats is an example of this problem: the categories are \"cat\" and \"no cat,\" which are mutually exclusive. (Keep in mind an image is stored in a computer as a numerical value representing the intensity of red, blue, and green colors in each pixel of the image, which may easily be stored as a very high-dimensional point.)\n",
    "\n",
    "Common methods for classification include logistic regresion, $k$-nearest neighbors, the Bayes and naive Bayes classifiers, discriminant analysis (LDA and QDA), decision trees, random forests, and support vector machines--all of which will be covered in this course--as well as neural networks (MLPs, CNNs, RNNs), which are beyond the scope of this course.\n",
    "\n",
    "### Regression Problems\n",
    "\n",
    "In a regression task, the learning algorithm tries to predict a numerical $m$-vector given an $d$-dimensional input example--e.g. to estimate a function $f:\\mathbb{R}^d\\to\\mathbb{R}^m$.\n",
    "\n",
    "An example is predicting the price for which a house will sell given information on the house--the number of bedrooms, the number of bathrooms, the floorspace, the size of the surrounding yard, whether or not it has a pool, etc. Here, as in many of the problems we will cover, $m=1$, meaning we will predict only one output variable.\n",
    "\n",
    "Common methods for regression include linear regression, ridge and LASSO regression, decision trees, random forest, and support vector machines--all of which will be covered in this course--as well as linear models and neural networks (MLPs, CNNs, RNNs), which are beyond the scope of this course. \n",
    "\n",
    "## Unsupervised Learning\n",
    "\n",
    "In unsupervised learning, we do not have the benefit of knowing some of the results we need to find. It is usually a somewhat less structured search for useful patterns in a dataset.\n",
    "\n",
    "### Clustering Problems\n",
    "\n",
    "A clustering task is one that tries to find which datapoints are similar to one another, in some sense that we do not necessarily define in advance.\n",
    "\n",
    "Common methods for clustering are K-means and hierarchical clustering--both of which we will cover in the course--as well as mean-shift clustering, DBSCAN, Gaussian mixture models, and self-organizing maps, which are beyond the scope of this course.\n",
    "\n",
    "### Dimensionality Reduction\n",
    "\n",
    "In a dimensionality reduction task, the goal is to represent a dataset using fewer features without losing patterns in \n",
    "\n",
    "For example, if we have a 1-megapixel color picture as a datapoint, it would have 1 million pixels and we would store three numbers (R, G, and B values) for each pixel, meaning the dimension of the image would be 3 million. It is sometimes far too slow to use datapoints in $\\mathbb{R}^{3000000}$ in machine learning algorithms. It is frequently helpful to find ways to store datapoints in lower-dimensional spaces such that important information is not lost. The idea is similar to compression.\n",
    "\n",
    "Common methods for dimensionality reduction are discriminant analysis (LDA and QDA), and principal components analysis (PCA)--which we will cover in the course--as well as autoencoders and word-embeddings, which are beyond the scope of this course.\n",
    "\n",
    "### Anomaly Detection\n",
    "\n",
    "In an anomaly detection task, the goal is to find unusual patterns in data.\n",
    "\n",
    "An example is credit card companies trying to detect unusual usage of a credit card. If they can detect unusual activity, they sometimes deactive credit cards to avoid fraud. False positives may cause problems for legitimate customers, but cards deactivated due to actual fraud can prevent further damage. \n",
    "\n",
    "### Denoising\n",
    "\n",
    "In denoising, the goal is to uncover some original dataset that has been corrupted by some sort of noise, whether we mean noisy sounds or random error. In all cases, the goal is to find a sometimes-faint signal within some noise.\n",
    "\n",
    "Denoising is not a task we will cover explicitly in the course.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* If you are speaking into a phone while riding in a car, the road noise can cause the voice signal not to be transmitted clearly\n",
    "* Noise-cancelling headphones try to counteract noise to pass through clear audio\n",
    "* Random errors in measurements in particle physics make the signal difficult to extract\n",
    "* Financial instruments like stocks can fluctuate at random while there is an underlying cause of general trends"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 (Wed, Jan 13)\n",
    "\n",
    "# Data Matrix\n",
    "\n",
    "In this section, we will cover how data is frequently stored such that it can be used by machine learning methods. It should not be thought that this is the only way or always the best way to store data, but it will be a series of conventions used in many fields.\n",
    "\n",
    "For many problems, we will have a **dataset** consisting of some number $n$ points in $\\mathbb{R}^d$ that we will store in a matrix\n",
    "\n",
    "$$\n",
    "X = \\begin{pmatrix}\n",
    "x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The $i$th *row* $x_i = (x_{i1}, x_{i2}, ..., x_{id})\\in\\mathbb{R}^d$ is the $i$th point in the dataset. These points have many names in different fields: **points, datapoints, examples, vectors, records, feature-vectors**. In some sources, these points $x_i$ are denoted $\\mathbf{x}_i$, but it should be clear that $x$ with a single subscripts indicates a point while $x$ with two subscripts is the component of a point.\n",
    "\n",
    "The $j$th *column* $X_j=(x_{1j}, x_{2j}, ..., x_{nj})\\in\\mathbb{R}^n$ is the $j$th component of each point in the dataset. These are likewise called by many names dependent on field: **features, attributes, variables, dimensions, properties, fields**. In some cases, each column can be considered a random sample of a random variable, or the rows of the matrix $X$ can be considered a random sample of vector-valued random variables.\n",
    "\n",
    "The number of points $n$ is the **size** of the dataset and the length of the points $d$ is called the **dimensionality** of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Problems\n",
    "\n",
    "**Regression problems** are problems where we try to predict a numerical output value given some input datapoint based on some examples ($x_i$, $y_i$) of input datapoints with known outputs or **targets**. This will be the task of the machine learning problem.\n",
    "\n",
    "## Examples\n",
    "\n",
    "1. If we have a dataset of workers in a certain position of numbers of years of experience as an input and salary, we might want to predict salaries we don't know based on the years of experience a person has.\n",
    "\n",
    "1. If we have a dataset of variables about houses (floorspace, number of bedrooms, number of bathrooms, number of stories, age of the house) along with their selling prices, we might want to predict selling prices of homes not in the dataset based on the other variables about the house.\n",
    "\n",
    "1. If we have a dataset of variables about countries (average salary, average education of citizens, death rate, birth rate, infant mortality rate, etc.) along with their GDP, we might want to predict the GDP of countires not in the dataset based on the other variables about the country.\n",
    "\n",
    "1. If we have a dataset of seasonal variables about NBA basketball teams (points per game, turnovers per game, point differential, rebounds per game, blocks per game, etc.) and the numbers of wins they had in different seasons, we might want to take the statistics of a team early in a season to try to predict the number of wins they will have and average number of points per game.\n",
    "\n",
    "## Types of Regression Problems\n",
    "\n",
    "There are different types of regression problems based on the numbers of input variables and output variables.\n",
    "\n",
    "* A **simple** regression problem predicts an output variable with just one input variable like Example 1.\n",
    "\n",
    "* A **multiple** regression problem predicts an output variable with more than one input variable like Examples 2 and 3.\n",
    "\n",
    "* A **multivariate** regression problem predicts more than one output variables like Example 4.\n",
    "\n",
    "## The Math of a Regression Problem\n",
    "\n",
    "All of these regression problems have some things in common: there are example datapoints with outputs and we want to predict the outputs for new datapoints. Consider a $d$-dimensional point, or vector, $x_i\\in\\mathbb{R}^d$ and denote $x_i=(x_{i1},x_{i2},...,x_{id})$. $x_i$ maps to an output $y_i\\in\\mathbb{R}^m$. (In statistics, the components of the vector $x_1$ are more frequently called **predictors** or **independent variables** and the $y_i$ values are more frequently called **responses** or **dependent variables**.)\n",
    "\n",
    "In a perfect world, a solution to a (univariate) regression problem will find a function $f:\\mathbb{R}^d\\to\\mathbb{R}$ that can do two things:\n",
    "\n",
    "1. Mapping each example $x_i$ in a dataset to its output $y_i=f(x_i)$\n",
    "1. Generalize to successfully predict outputs of new datapoints\n",
    "\n",
    "In reality, $f$ will not always map each input $x_i$ values to each $y_i$ value perfectly or generalize to new inputs well, but we try to get as close to these ideals as possible.\n",
    "\n",
    "## Regression Algorithms\n",
    "\n",
    "There are a number of popular approaches to regression problems, including the following.\n",
    "\n",
    "* linear regression\n",
    "* lasso regression\n",
    "* ridge regression\n",
    "* decision trees\n",
    "* support vector machines\n",
    "* neural networks\n",
    "\n",
    "In the near term, we will consider the first three approaches as they are quite similar. As an added bonus, they all use a numerical optimization scheme called gradient descent, which is one of the main algorithms of machine learning.\n",
    "\n",
    "## Linear Regression by Ordinary Least Squares\n",
    "\n",
    "We will assume the function $f$ we aim to predict is linear in some parameters $\\beta_0,...,\\beta_d$ so that our predicted function will be\n",
    "\n",
    "$$\n",
    "f(x_i)=\\beta_0 + \\sum\\limits_{k=1}^d \\beta_k x_{ik}=\\beta_0+\\beta_1 x_{i1}+\\cdots+\\beta_d x_{id}\n",
    "$$\n",
    "\n",
    "for some parameters $\\beta_0,...,\\beta_d$. Here, we assume the output $Y$ is a random variable of the form\n",
    "\n",
    "$$Y = f(X_1, ..., X_d) + \\varepsilon$$\n",
    "\n",
    "where $\\varepsilon$ is a random error term that is assumed to account for the effect of unknown latent variables and intrinsic randomness of $y$\n",
    "\n",
    "We will aim to choose the values of $\\beta_0,...,\\beta_d$ that will minimize a loss function on a training dataset $(x_1,y_1),...,(x_n,y_n)$. This loss function will be small if each $f(x_i)$ is near each $y_i$, which is what we want.\n",
    "\n",
    "### Note\n",
    "\n",
    "It is a common misconception that \"linear regression\" must fit linear functions to data, but the \"linear\" part of linear regression refers to the fact that $f$ is linear with respect to $\\beta_0,...,\\beta_d$, not with respect to $x_i$, so it is certainly possible to apply some preprocessing to the datapoints, which in effect, fits a nonlinear surface.\n",
    "\n",
    "For example, if each $x_i\\in\\mathbb{R}^1$, we can fit a parabola by manipulating each $x_i$ into $x_i^*=(x_i,x_i^2)$ so that our predicted function would be\n",
    "\n",
    "$$f(x_i)=\\beta_0+\\beta_1 x_i+\\beta_2 x_i^2$$\n",
    "\n",
    "While we will discuss only points in the form $x_i=(x_{i1},...,x_{id})$ for now, keep in mind that we can always create new variables from the data, preprocess the data into different forms, and so on, to consider some (kernel) function of the data $g(x_i)$ as the inputs, as long as the function is differentiable (or at least piecewise differentiable). The main point here is that linear regression can learn to represent functions far beyond simply lines and planes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression by Ordinary Least Squares (OLS)\n",
    "\n",
    "Recall that we have a labeled dataset $(x_1,y_1)$, ..., $(x_n,y_n)$ with each $x_i\\in\\mathbb{R}^d$ and each $y_i\\in\\mathbb{R}$ and our goal is to find a function $f$ that maps the $x_i$'s to the $y_i$'s as well as possible, and, we hope, is effective at mapping unknown datapoints to appropriate outputs.\n",
    "\n",
    "In the **ordinary least squares** method, we try to minimize a the **sum of squared differences** between the real outputs $y_1, ..., y_n$ and the predicted outputs $f(x_1)$, ..., $f(x_n)$. In other words, the loss function in this method is\n",
    "\n",
    "$$L(\\beta)=\\sum\\limits_{i=1}^n \\left(f(x_i)-y_i\\right)^2 = \\sum\\limits_{i=1}^n \\left(x_i^T\\beta-y_i\\right)^2,$$\n",
    "\n",
    "which we will call a **loss function**, where\n",
    "\n",
    "$$\n",
    "X=\\begin{pmatrix}\n",
    "1 & x_{11} & x_{12} & \\cdots & x_{1d}\\\\\n",
    "1 & x_{21} & x_{22} & \\cdots & x_{2d}\\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
    "1 & x_{n1} & x_{n2} & \\cdots & x_{nd}\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}y=\\begin{pmatrix}\n",
    "y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_n\n",
    "\\end{pmatrix}\n",
    "\\hspace{2cm}\\beta=\\begin{pmatrix}\n",
    "\\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_d\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "This matrix is the same as what we used above, except we added a column of ones to the left for convenience.\n",
    "\n",
    "### Norms and Distance Metrics\n",
    "\n",
    "Let $d(x_1,x_2)=\\|x_1-x_2\\|$ be the distance from $x_1$ to $x_2$. The specific formula we use is called the **distance metric** and the function $\\|\\cdot\\|:\\mathbb{R}^d\\to[0,\\infty)$ is called a **norm**. The most-used norms are from the family of $L^p$ distances for each number $p>0$. (Note that mathematicians would likely refer to this as simply the $p$-norm for a finite dimensional space.) Distances between points are computed with the $L^p$-norm as\n",
    "\n",
    "$$\\left\\|x_1-x_2\\right\\|_p =\\left(\\sum\\limits_{i=1}^d |x_{1i}-x_{2i}|^p\\right)^{1/p}$$\n",
    "\n",
    "* If $p=1$, we get what is called the **taxi-cab** or **Manhattan distance** because, geometrically, the distance from (4, 6) to (8, 15) is the distance to drive along city blocks from 4th street and 6th avenue to 8th street and 15th avenue. It is computed as\n",
    "\n",
    "$$\\left\\|x_1-x_2\\right\\|_1= \\left|x_{11}-x_{21}\\right|+\\cdots+\\left|x_{1d}-x_{2d}\\right|$$\n",
    "\n",
    "* If $p=2$, we get the familiar **Euclidean distance**, the straight-line distance in flat space you have certainly encountered in elementary algebra and calculus courses,\n",
    "\n",
    "$$\n",
    "\\left\\|x_1-x_2\\right\\|_2=\\sqrt{\\left(x_{11}-x_{21}\\right)^2+\\cdots+\\left(x_{1d}-x_{2d}\\right)^2}$$\n",
    "\n",
    "* If $p=\\infty$, we get what is called the **supremum** or **maximum distance** and it is calculated as\n",
    "\n",
    "$$\\left\\|x_1-x_2\\right\\|_\\infty= \\max\\limits_{i=1,...,d}\\left|x_{1i}-x_{2i}\\right|=\\max\\{|x_{11}-x_{21}|, ..., |x_{1d}-x_{2d}|\\}$$\n",
    "\n",
    "For now, let's we will use the Euclidean distance, but in other areas, different $L^p$ norms may be used as may other <a href=\"https://en.wikipedia.org/wiki/Norm_(mathematics)#Definition\">norms</a> (see 2.5 in Goodfellow et. al.).\n",
    "\n",
    "## Back to OLS\n",
    "\n",
    "Given this, we can rewrite the loss function as\n",
    "\n",
    "$$L(\\beta)=\\|X\\beta-y\\|^2_2=(X\\beta-y)^T(X\\beta-y),$$\n",
    "\n",
    "where the $T$ subscript represents the **transpose** of a matrix. Here, $X$ and $y$ are constants derived from the dataset, so the $\\beta$ values are the only unknowns, so we need to find the $\\beta$ values that minimize the loss function $L$. In other words, we need to solve a minimization problem\n",
    "\n",
    "$$\\min\\limits_\\beta\\,L(\\beta)=\\min\\limits_\\beta\\,(X\\beta - y)^T(X\\beta - y).$$\n",
    "\n",
    "We should note that these $\\beta$ values are called **parameters** of the model, which are values that are estimated by the model automatically from the data.\n",
    "\n",
    "In multivariate calculus, the approach to minimize a differentiable function with unbounded domain is to take derivatives with respect to $\\beta_0, ..., \\beta_d$, set them all equal to zero, solve, and compare the function at these critical values. This may seem difficult, but we can actually do it simply for ordinary least squares. Before we take derivatives, let's convert the loss function into a (longer but) simpler form. Transposes can be applied to sums of matrices separately, so\n",
    "\n",
    "$$L(\\beta)=((X\\beta)^T-y^T)(X\\beta-y).$$\n",
    "\n",
    "Using the distributive property of matrix multiplication,\n",
    "\n",
    "$$L(\\beta)=(X\\beta)^T X\\beta-(X\\beta)^T y-y^T X\\beta+y^T y$$\n",
    "\n",
    "If we realize $y$ and $X\\beta$ are both matrices of shape $n\\times 1$, then we should have $(X\\beta)^T y=y^T X\\beta$, so the loss function is\n",
    "\n",
    "$$L(\\beta)=(X\\beta)^T X\\beta-2(X\\beta)^T y+y^T y$$\n",
    "\n",
    "Since $(AB)^T=B^TA^T$ for matrices, we can simplify the terms as\n",
    "\n",
    "$$L(\\beta)=\\beta^T X^T X\\beta-2\\beta^T X^T y+y^T y$$\n",
    "\n",
    "Now, of course, this is a scalar (because, in the end, the loss is just a number--the sum of squared errors), so we can take the derivatives with respect to $\\beta_1$, ..., $\\beta_d$ and put those into a vector as\n",
    "\n",
    "$$\\nabla L(\\beta)=2X^T X\\beta-2X^T y$$\n",
    "\n",
    "As in multivariate calculus, we set this whole vector equal to 0 and solve for the estimated version of $\\beta$ as\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "2X^T X\\beta-2X^T y &= 0\\\\\n",
    "X^T X\\beta &= X^T y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Then, if $X^T X$ is an invertible matrix, then we can multiply both sides of the equation by its inverse to solve for $\\beta$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "(X^T X)^{-1}(X^T X\\beta) &= (X^T X)^{-1}X^T y\\\\\n",
    "\\beta &=(X^T X)^{-1}X^T y.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "At long last, we have a formula for the exact solution for the $\\beta$ values that minimize the loss function.\n",
    "\n",
    "#### Linear Algebra Notes\n",
    "\n",
    "* The formula above holds only if the inverse of the matrix $X^T X$ exists. Assuming $n\\geq d$, the inverse exists when the columns of $X$ are linearly independent. (See <a href=\"https://www.khanacademy.org/math/linear-algebra/matrix-transformations/matrix-transpose/v/lin-alg-showing-that-a-transpose-x-a-is-invertible\">this video</a> or pretty much any linear algebra book.)\n",
    "\n",
    "* If you have studied linear algebra, you might recognize $(X^T X)^{-1}X^T$ is the <a href=\"https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html\">Moore-Penrose pseudoinverse</a> of $X$.\n",
    "\n",
    "Anyway, the formula for $\\beta$ does not look so nice to do by hand since it requires a matrix multiplication, a matrix inverse, and two more matrix multiplications, but a computer can complete these tasks quickly. (The <a href=\"https://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html\">best algorithms</a> for matrix multiplication and matrix inverses for $n\\times n$ matrices each have computational complexity less than $O(n^3)$, so doing a few of these is no problem, even for quite large matrices.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 (Fri, Jan 15)\n",
    "\n",
    "## Training and Testing a Regression Model\n",
    "\n",
    "If we have a dataset of labeled data, a very common approach is to randomly split the dataset into two parts: the training set and the testing set. We remove the labels from the test set, \"train\" the model with the training set, use the resulting model to attempt to make predictions for the test set, and then measure performance.\n",
    "\n",
    "There are no strict rules here, but it is common to use 60\\% train 40\\% test, although folks like Andrew Ng have argued that a much smaller test sets are reasonable if the dataset is very large--e.g. datasets with millions of datapoints are not uncommon in some fields. This way, we can measure the success of our regression model on data it has never seen (the testing set). Once we become confident our model works well in this way, we can be more confident that it will **generalize** well in the real world.\n",
    "\n",
    "My preferred approach is to use the <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\">train_test_split</a> function from the <a href=\"https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\">scikit-learn.model_selection</a> library to randomly assign a specified percentage (usually 50-75%) of the dataset to the training set and the rest in the test set.\n",
    "\n",
    "### Hyperparameters and Dev Sets\n",
    "\n",
    "In some models, there are **hyperparameters** to tune. These are settings the user specifies before running the algorithms which may have an impact on performance. In this case, the test set is frequently split in half into \"dev\" and \"test\" sets. The dev set is used for tuning the hyperparameters before testing the model on unknown test sets. More on that later.\n",
    "\n",
    "## Performance Metrics for Regression\n",
    "\n",
    "Let's consider a few performance metrics in common usage for regression.\n",
    "\n",
    "With linear regression, we generally have the unfortunate situation that the model we construct is not perfect even on the training set. Therefore, we first need to consider the performance on the training data to which it was fit just to see how well the model fits to the training data. The formulas for the common metrics are not particularly interesting, so we just state what they represent and what value they should ideally have:\n",
    "\n",
    "* **Coefficient of determination** $R^2$ - the fraction of the variation in the data explained by the model. It is, in a sense, a measure of the strength of the linear relationship between the variables. Ideally, it will be near 1.\n",
    "\n",
    "* **Sum of squared error (SSE)** - the loss function we have used. Ideally, it will be as small as possible.\n",
    "\n",
    "* **Mean squared error (MSE)** is simply the SSE divided by the number of examples we test. It is frequently better because it makes little sense to measure success in a way that is so depedent on the dataset size. Ideally, it should of course be small.\n",
    "\n",
    "* **Mean absolute error (MAE)** - the mean of the absolute errors between the points and the fitted function. Ideally, it will be as small as possible.\n",
    "\n",
    "If these values are far from their ideal values for the training set, the model does not even fit the training data well, so it probably will not fit the testing data well. The ordinary least squares solution finds the optimal parameters for a linear fit, so poor performance on the training set means the data do not have a strong linear relationship.\n",
    "\n",
    "Some preprocessing of the data might make it work better. For example, you can apply a logarithm to a variable if there's a linear relationship with that variable on a log scale. See the (free) classic book <a href=\"https://web.stanford.edu/~hastie/ElemStatLearn/\">*Elements of Statistical Learning*</a> by Hastie, et. al., section 2.6.3 for an introduction on linear basis expansions.\n",
    "\n",
    "Second, we need to consider the performance on the testing data. We generally should consider the MSE or MAE.\n",
    "\n",
    "## Ordinary Least Squares Code\n",
    "\n",
    "Before we write some code for ordinary least squares, let's import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# increase the width of boxes in the notebook file (this is only cosmetic)\n",
    "np.set_printoptions(linewidth=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a class for using ordinary least squares in this way to fit the model and to predict outputs for unknown inputs. We will use the scikit-learn pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrdinaryLeastSquaresExact:\n",
    "        \n",
    "    # fit the model to the data\n",
    "    def fit(self, X, y):\n",
    "        # save the training data\n",
    "        self.data = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # save the training labels\n",
    "        self.outputs = y\n",
    "        \n",
    "        # find the beta values that minimize the sum of squared errors\n",
    "        X = self.data\n",
    "        self.beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "                \n",
    "    # predict the output from input (testing) data\n",
    "    def predict(self, X):\n",
    "        # initialize an empty matrix to store the predicted outputs\n",
    "        yPredicted = np.empty([X.shape[0],1])\n",
    "        \n",
    "        # append a column of ones at the beginning of X\n",
    "        X = np.hstack((np.ones([X.shape[0],1]), X))\n",
    "        \n",
    "        # apply the function f with the values of beta from the fit function to each testing datapoint\n",
    "        for row in range(X.shape[0]):\n",
    "            yPredicted[row] = self.beta @ X[row,]\n",
    "            \n",
    "        return yPredicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1D Example\n",
    "\n",
    "Let's make up some 1D data and test the method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted y values are [1.2 1.9 2.6 3.3 4. ]\n",
      "The real y values are [1 2 3 3 4]\n",
      "The beta values are [-3.   0.7]\n",
      "The r^2 score is 0.9423076923076923\n",
      "The mean squared error is 0.06\n",
      "The mean absolute error is 0.20000000000000054\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = np.array([[6], [7], [8], [9], [10]])\n",
    "y = np.array([1, 2, 3, 3, 4])\n",
    "\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "model.fit(X,y)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values are', y)\n",
    "\n",
    "# print the beta values\n",
    "parameters = model.beta\n",
    "print('The beta values are', parameters)\n",
    "\n",
    "# plot the training points\n",
    "plt.scatter(X, y, label = 'Training Data')\n",
    "\n",
    "# plot the fitted model with the training data\n",
    "xModel = np.linspace(6,10,100)\n",
    "yModel = parameters[0] + parameters[1]*xModel\n",
    "lineFormula = 'y={:.3f}+{:.3f}x'.format(parameters[0], parameters[1])\n",
    "plt.plot(xModel, yModel, 'r', label = lineFormula)\n",
    "\n",
    "# add a legend\n",
    "plt.legend()\n",
    "\n",
    "# return quality metrics\n",
    "print('The r^2 score is', r2_score(y, predictions))\n",
    "print('The mean squared error is', mean_squared_error(y, predictions))\n",
    "print('The mean absolute error is', mean_absolute_error(y, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Example\n",
    "\n",
    "Let's make up some 2D data and test to see if our method works well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.9642679900744417\n",
      "The mean squared error on the training set is 4.607999999999994\n",
      "The mean absolute error on the training set is 1.5360000000001235\n",
      "The predicted y values for the test set are [-6.52 19.08  6.6  32.2 ]\n",
      "The real y values for the test set are [ 9 15 25 31]\n",
      "The beta values are [-3.56 -6.24  9.52]\n",
      "The mean squared error on the test set is 149.37919999999122\n",
      "The mean absolute error on the test set is 9.799999999999963\n"
     ]
    }
   ],
   "source": [
    "trainX = np.array([[2, 2], [2, 3], [5, 6], [6, 7], [9, 10]])\n",
    "trainY = np.array([3, 13, 19, 29, 35])\n",
    "\n",
    "testX = np.array([[2, 1], [4, 5], [6, 5], [8, 9]])\n",
    "testY = np.array([9, 15, 25, 31])\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', predictions.T[0])\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a low dimensional problem like this one, we can plot the points along with the function $f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'y')"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "n = 100\n",
    "\n",
    "# scatter plot of the test data\n",
    "x1 = testX[:,0]\n",
    "x2 = testX[:,1]\n",
    "y = testY\n",
    "ax.scatter(x1, x2, y, marker = 'o')\n",
    "\n",
    "# scatter plot of the training data\n",
    "x1 = trainX[:,0]\n",
    "x2 = trainX[:,1]\n",
    "y = trainY\n",
    "ax.scatter(x1, x2, y, marker = '^')\n",
    "\n",
    "# plot the plane we fit to the data\n",
    "beta = model.beta\n",
    "\n",
    "# surface plot\n",
    "x1 = np.linspace(0,10,10)\n",
    "x2 = np.linspace(0,10,10)\n",
    "\n",
    "X1, X2 = np.meshgrid(x1,x2)\n",
    "#Y = beta[0]*X1 + beta[1]*X2\n",
    "Y = beta[0] + beta[1]*X1 + beta[2]*X2\n",
    "\n",
    "surf = ax.plot_wireframe(X1, X2, Y)\n",
    "ax.view_init(30, 40)\n",
    "\n",
    "# add axis labels\n",
    "ax.set_xlabel('x_1')\n",
    "ax.set_ylabel('x_2')\n",
    "ax.set_zlabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: High School Graduation Rates in US States\n",
    "\n",
    "Let's try to use ordinary least squares on a real dataset. The CSV file in '/data/US_State_data.csv' contains data from each U.S. state.\n",
    "\n",
    "We would like to predict the output variable included, the high school graduation rate, from some input variables: including the crime rate (per 100,000 persons), the violent crime rate (per 100,000 persons), average teacher salary, student-to-teacher ratio, education expenditure per student, population density, and median household income.\n",
    "\n",
    "This means we have 50 examples (one for each state), 7 input (predictor) variables, and one output (response) variable. In order to use the formula we derived above to attack the problem with ordinary least squares, we need to find the matrices $X$ and $y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.3921626378110835\n",
      "The mean squared error on the training set is 19.59820392039355\n",
      "The mean absolute error on the training set is 3.796456104985723\n",
      "The predicted y values for the test set are [80. 80. 89. 89. 80. 81. 85. 93. 75. 84. 84. 79. 83.]\n",
      "The real y values for the test set are      [70. 83. 83. 81. 76. 87. 89. 89. 78. 78. 84. 80. 79.]\n",
      "The beta values are [ 1.16286460e+02 -6.09008374e-03  3.98286852e-03 -1.63697058e-04 -2.97328939e-01 -4.74750536e-04  1.09164594e-02]\n",
      "The mean squared error on the test set is 28.314919261803976\n",
      "The mean absolute error on the test set is 4.643935838705472\n"
     ]
    }
   ],
   "source": [
    "# import the data from the csv file to an numpy array\n",
    "data = pd.read_csv('data/US_State_Data.csv', sep=',').to_numpy()\n",
    "\n",
    "# select the data and the labels\n",
    "X = np.array(data[:,1:7], dtype=float)\n",
    "y = np.array(data[:,8], dtype=float)\n",
    "\n",
    "# split the data into training and test sets\n",
    "(trainX, testX, trainY, testY) = train_test_split(X, y, test_size = 0.25, random_state = 1)\n",
    "\n",
    "# run the model (same code as above)\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = OrdinaryLeastSquaresExact()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', r2_score(trainY, trainPredictions))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the training set is', mean_squared_error(trainY, trainPredictions))\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the predictions\n",
    "print('The predicted y values for the test set are', np.round(predictions.T[0],0))\n",
    "\n",
    "# print the real y values\n",
    "print('The real y values for the test set are     ', testY)\n",
    "\n",
    "# print the beta values\n",
    "print('The beta values are', model.beta)\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean squared error on the test set is', mean_squared_error(testY, predictions))\n",
    "print('The mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Multivariate Regression for Air Quality \n",
    "\n",
    "For this example, we will use the Beijing Multi-Site Air-Quality Data Data Set dataset$^1$ available from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Beijing+Multi-Site+Air-Quality+Data). It is a hourly data set considers 6 main air pollutants and 6 relevant meteorological variables at multiple sites in Beijing.\n",
    "\n",
    "[1] Zhang, S., Guo, B., Dong, A., He, J., Xu, Z. and Chen, S.X. (2017) Cautionary Tales on Air-Quality Improvement in Beijing. *Proceedings of the Royal Society A*, Volume 473, No. 2205. https://doi.org/10.1098/rspa.2017.0457\n",
    "\n",
    "The dataset includes a number of variables associated with time, weather, and location: the year, month, day, hour, temperature, pressure, dew point, precipitation, wind direction, wind speed, and station name.\n",
    "\n",
    "The dataset also includes air pollutant levels for: fine inhalable particulate matter with diameter $\\leq 2.5\\,\\mu$m (PM$_{2.5}$), inhalable particles with diameter $\\leq 10\\, \\mu$m (PM$_{10}$), sulfur dioxide (SO$_2$), nitrogen dioxide (NO$_2$), carbon monoxide (CO), and ozone (O$_3$).\n",
    "\n",
    "We will try to predict these pollution levels based the time, weather, and location variables.\n",
    "\n",
    "This dataset is not as neat as the US high school graduation data: there are numerical variables, but there are a few new wrinkles:\n",
    "\n",
    "* Some variables are text\n",
    "* There is missing numbers in the dataset\n",
    "* There is a variable that simply indexes the data\n",
    "\n",
    "All of these issues would prevent us from using ordinary least squares, so we need to solve these issues. In most applied problems, there are similar issues that must be managed before you can do much machine learning.\n",
    "\n",
    "### Cleaning/Preprocessing the Data\n",
    "\n",
    "First, we must **clean** the data (manipulate it into a data matrix we can use for machine learning). The data is stored in 12 separate comma-separated value (CSV) files, so we will use the `glob` library to iterate through the files and use the `pandas` library to store each as a dataframe and then concatenate them into one big dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>No</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>35060</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>35061</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1013.6</td>\n",
       "      <td>-15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>35062</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>35063</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>35064</td>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>420768 rows  18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          No  year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  \\\n",
       "0          1  2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0   \n",
       "1          2  2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0   \n",
       "2          3  2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0   \n",
       "3          4  2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0   \n",
       "4          5  2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0   \n",
       "...      ...   ...    ...  ...   ...    ...   ...   ...   ...    ...   ...   \n",
       "35059  35060  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0   \n",
       "35060  35061  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0   \n",
       "35061  35062  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0   \n",
       "35062  35063  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0   \n",
       "35063  35064  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0   \n",
       "\n",
       "       TEMP    PRES  DEWP  RAIN   wd  WSPM        station  \n",
       "0      -0.7  1023.0 -18.8   0.0  NNW   4.4   Aotizhongxin  \n",
       "1      -1.1  1023.2 -18.2   0.0    N   4.7   Aotizhongxin  \n",
       "2      -1.1  1023.5 -18.2   0.0  NNW   5.6   Aotizhongxin  \n",
       "3      -1.4  1024.5 -19.4   0.0   NW   3.1   Aotizhongxin  \n",
       "4      -2.0  1025.2 -19.5   0.0    N   2.0   Aotizhongxin  \n",
       "...     ...     ...   ...   ...  ...   ...            ...  \n",
       "35059  12.5  1013.5 -16.2   0.0   NW   2.4  Wanshouxigong  \n",
       "35060  11.6  1013.6 -15.1   0.0  WNW   0.9  Wanshouxigong  \n",
       "35061  10.8  1014.2 -13.3   0.0   NW   1.1  Wanshouxigong  \n",
       "35062  10.5  1014.4 -12.9   0.0  NNW   1.2  Wanshouxigong  \n",
       "35063   8.6  1014.1 -15.9   0.0  NNE   1.3  Wanshouxigong  \n",
       "\n",
       "[420768 rows x 18 columns]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# import all the files in data/PRSA\n",
    "dataFiles = glob.glob('data/PRSA/*')\n",
    "\n",
    "# empty list to store the data\n",
    "data = []\n",
    "\n",
    "# iterate through files, read files\n",
    "for file in dataFiles:\n",
    "    # convert file to dataframe and add it to the list\n",
    "    data.append(pd.read_csv(file, sep = ','))\n",
    "    \n",
    "# concatenate all the dataframes into one\n",
    "data = pd.concat(data)\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can use pandas to drop unimportant variables:\n",
    "\n",
    "* Drop the '**No**' column with `drop()` since it just stores an index that has no physical significance.\n",
    "* Drop rows with empty values with `dropna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>TEMP</th>\n",
       "      <th>PRES</th>\n",
       "      <th>DEWP</th>\n",
       "      <th>RAIN</th>\n",
       "      <th>wd</th>\n",
       "      <th>WSPM</th>\n",
       "      <th>station</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>1023.0</td>\n",
       "      <td>-18.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>4.4</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.2</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>4.7</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1023.5</td>\n",
       "      <td>-18.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>5.6</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>1024.5</td>\n",
       "      <td>-19.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>3.1</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>1025.2</td>\n",
       "      <td>-19.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>N</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Aotizhongxin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>1013.5</td>\n",
       "      <td>-16.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>11.6</td>\n",
       "      <td>1013.6</td>\n",
       "      <td>-15.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>WNW</td>\n",
       "      <td>0.9</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>10.8</td>\n",
       "      <td>1014.2</td>\n",
       "      <td>-13.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NW</td>\n",
       "      <td>1.1</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>1014.4</td>\n",
       "      <td>-12.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNW</td>\n",
       "      <td>1.2</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>8.6</td>\n",
       "      <td>1014.1</td>\n",
       "      <td>-15.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NNE</td>\n",
       "      <td>1.3</td>\n",
       "      <td>Wanshouxigong</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382168 rows  17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  TEMP  \\\n",
       "0      2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  -0.7   \n",
       "1      2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  -1.1   \n",
       "2      2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  -1.1   \n",
       "3      2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  -1.4   \n",
       "4      2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  -2.0   \n",
       "...     ...    ...  ...   ...    ...   ...   ...   ...    ...   ...   ...   \n",
       "35059  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0  12.5   \n",
       "35060  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0  11.6   \n",
       "35061  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0  10.8   \n",
       "35062  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0  10.5   \n",
       "35063  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0   8.6   \n",
       "\n",
       "         PRES  DEWP  RAIN   wd  WSPM        station  \n",
       "0      1023.0 -18.8   0.0  NNW   4.4   Aotizhongxin  \n",
       "1      1023.2 -18.2   0.0    N   4.7   Aotizhongxin  \n",
       "2      1023.5 -18.2   0.0  NNW   5.6   Aotizhongxin  \n",
       "3      1024.5 -19.4   0.0   NW   3.1   Aotizhongxin  \n",
       "4      1025.2 -19.5   0.0    N   2.0   Aotizhongxin  \n",
       "...       ...   ...   ...  ...   ...            ...  \n",
       "35059  1013.5 -16.2   0.0   NW   2.4  Wanshouxigong  \n",
       "35060  1013.6 -15.1   0.0  WNW   0.9  Wanshouxigong  \n",
       "35061  1014.2 -13.3   0.0   NW   1.1  Wanshouxigong  \n",
       "35062  1014.4 -12.9   0.0  NNW   1.2  Wanshouxigong  \n",
       "35063  1014.1 -15.9   0.0  NNE   1.3  Wanshouxigong  \n",
       "\n",
       "[382168 rows x 17 columns]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop the 'No' column\n",
    "data = data.drop(columns = ['No'])\n",
    "\n",
    "# drop rows with missing data\n",
    "data = data.dropna()\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Converting a Categorical Variable to One-Hot\n",
    "\n",
    "Note the number of datapoints went from 420768 to 382168, a loss of about 9\\% of the data, due to some data being incomplete. Simply dropping datapoints can sometimes bias the model, but this is a relatively small amount of data, so it should not be a big problem.\n",
    "\n",
    "The next problem we have is that the **station** variable is not numerical but is rather categorical, representing the site where the datapoint was measured. This does not work with ordinary least squares. We could delete them from the dataset as well, but these sites are in different parts of the city which may experience different pollution patterns, so this information may help the model make predictions.\n",
    "\n",
    "One way to deal with categorical variables in machine learning problems is to convert them to **one-hot vectors** which are standard basis vectors of $\\mathbb{R}^m$ where $m$ is the number of categories. In other words, they are made up of all 0s except for one 1, representing the one category in the datapoint.\n",
    "\n",
    "Luckily, `Pandas` has a `get_dummies()` function, which does this conversion for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>...</th>\n",
       "      <th>station_Dingling</th>\n",
       "      <th>station_Dongsi</th>\n",
       "      <th>station_Guanyuan</th>\n",
       "      <th>station_Gucheng</th>\n",
       "      <th>station_Huairou</th>\n",
       "      <th>station_Nongzhanguan</th>\n",
       "      <th>station_Shunyi</th>\n",
       "      <th>station_Tiantan</th>\n",
       "      <th>station_Wanliu</th>\n",
       "      <th>station_Wanshouxigong</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382168 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  ...  \\\n",
       "0      2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  ...   \n",
       "1      2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  ...   \n",
       "2      2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  ...   \n",
       "3      2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  ...   \n",
       "4      2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  ...   \n",
       "...     ...    ...  ...   ...    ...   ...   ...   ...    ...   ...  ...   \n",
       "35059  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0  ...   \n",
       "35060  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0  ...   \n",
       "35061  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0  ...   \n",
       "35062  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0  ...   \n",
       "35063  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0  ...   \n",
       "\n",
       "       station_Dingling  station_Dongsi  station_Guanyuan  station_Gucheng  \\\n",
       "0                     0               0                 0                0   \n",
       "1                     0               0                 0                0   \n",
       "2                     0               0                 0                0   \n",
       "3                     0               0                 0                0   \n",
       "4                     0               0                 0                0   \n",
       "...                 ...             ...               ...              ...   \n",
       "35059                 0               0                 0                0   \n",
       "35060                 0               0                 0                0   \n",
       "35061                 0               0                 0                0   \n",
       "35062                 0               0                 0                0   \n",
       "35063                 0               0                 0                0   \n",
       "\n",
       "      station_Huairou  station_Nongzhanguan  station_Shunyi  station_Tiantan  \\\n",
       "0                   0                     0               0                0   \n",
       "1                   0                     0               0                0   \n",
       "2                   0                     0               0                0   \n",
       "3                   0                     0               0                0   \n",
       "4                   0                     0               0                0   \n",
       "...               ...                   ...             ...              ...   \n",
       "35059               0                     0               0                0   \n",
       "35060               0                     0               0                0   \n",
       "35061               0                     0               0                0   \n",
       "35062               0                     0               0                0   \n",
       "35063               0                     0               0                0   \n",
       "\n",
       "       station_Wanliu  station_Wanshouxigong  \n",
       "0                   0                      0  \n",
       "1                   0                      0  \n",
       "2                   0                      0  \n",
       "3                   0                      0  \n",
       "4                   0                      0  \n",
       "...               ...                    ...  \n",
       "35059               0                      1  \n",
       "35060               0                      1  \n",
       "35061               0                      1  \n",
       "35062               0                      1  \n",
       "35063               0                      1  \n",
       "\n",
       "[382168 rows x 28 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the 'station' column to binary variables\n",
    "data = pd.get_dummies(data, columns = ['station'])\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the station variable has been replaced with a binary variable for each station. In all, we lost 1 column (**station**) and gained 12 more, **station_(station name)**, resulting in 28 total columns.\n",
    "\n",
    "#### Wind Direction\n",
    "\n",
    "The last problem we see is the wind direction column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        NNW\n",
       "1          N\n",
       "2        NNW\n",
       "3         NW\n",
       "4          N\n",
       "        ... \n",
       "35059     NW\n",
       "35060    WNW\n",
       "35061     NW\n",
       "35062    NNW\n",
       "35063    NNE\n",
       "Name: wd, Length: 382168, dtype: object"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['wd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These values in the **wd** column represent the wind speed. So, why not simply replace them with one-hot vectors like the **station** column?\n",
    "\n",
    "We could, but this obscures some information in the data. These are stored as categorical variables, but they correspond to angles. If we used one-hot vectors, we may lose that physical structure.\n",
    "\n",
    "Another option is to simply convert them to angles and store them in radians as $\\theta=0$, $\\frac{\\pi}{8}$, $\\frac{2\\pi}{8}$, ..., $\\frac{15\\pi}{8}$. (In reality, different angles are possible, but the original data was rouded to these values.) This leads to another problem: here, it will appear to the algorithm that, for example, the difference between an angles of $0$ and $\\frac{15\\pi}{8}$ will be much greater than the difference between angles of $0$ and $\\pi$, which is not quite right in this context.\n",
    "\n",
    "A better solution is to store the wind direction as a vector on the unit circle as $(\\cos(\\theta), \\sin(\\theta))$. So, let's write a simple function that converts radian measures to this vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an angle to a list of unit circle coordinates\n",
    "def unitCircle(angle):\n",
    "    return [np.cos(angle), np.sin(angle)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's make a dictionary that maps each direction to the appropriate angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unitX</th>\n",
       "      <th>unitY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>E</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ENE</th>\n",
       "      <td>9.238795e-01</td>\n",
       "      <td>3.826834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NE</th>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNE</th>\n",
       "      <td>3.826834e-01</td>\n",
       "      <td>9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>N</th>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NNW</th>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NW</th>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WNW</th>\n",
       "      <td>-9.238795e-01</td>\n",
       "      <td>3.826834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>W</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.224647e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>WSW</th>\n",
       "      <td>-9.238795e-01</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SW</th>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSW</th>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>-9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>S</th>\n",
       "      <td>-1.836970e-16</td>\n",
       "      <td>-1.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SSE</th>\n",
       "      <td>3.826834e-01</td>\n",
       "      <td>-9.238795e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SE</th>\n",
       "      <td>7.071068e-01</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ESE</th>\n",
       "      <td>9.238795e-01</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            unitX         unitY\n",
       "E    1.000000e+00  0.000000e+00\n",
       "ENE  9.238795e-01  3.826834e-01\n",
       "NE   7.071068e-01  7.071068e-01\n",
       "NNE  3.826834e-01  9.238795e-01\n",
       "N    6.123234e-17  1.000000e+00\n",
       "NNW -3.826834e-01  9.238795e-01\n",
       "NW  -7.071068e-01  7.071068e-01\n",
       "WNW -9.238795e-01  3.826834e-01\n",
       "W   -1.000000e+00  1.224647e-16\n",
       "WSW -9.238795e-01 -3.826834e-01\n",
       "SW  -7.071068e-01 -7.071068e-01\n",
       "SSW -3.826834e-01 -9.238795e-01\n",
       "S   -1.836970e-16 -1.000000e+00\n",
       "SSE  3.826834e-01 -9.238795e-01\n",
       "SE   7.071068e-01 -7.071068e-01\n",
       "ESE  9.238795e-01 -3.826834e-01"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# list all wind directions, along the unit circle\n",
    "directions = ['E', 'ENE', 'NE', 'NNE', 'N', 'NNW', 'NW', 'WNW', 'W', 'WSW', 'SW', 'SSW', 'S', 'SSE', 'SE', 'ESE']\n",
    "\n",
    "# make a dictionary associating each direction with coordinates on the unit circle \n",
    "directionDict = {direction : unitCircle(i*np.pi/8) for (i, direction) in enumerate(directions)}\n",
    "\n",
    "# create a dataframe from the dictionary\n",
    "directionDf = pd.DataFrame.from_dict(directionDict, orient = 'index', columns = ['unitX', 'unitY'])\n",
    "\n",
    "# display the dataframe\n",
    "directionDf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal will be to add **unitX** and **unitY** columns to our `data` dataframe, map the existing **wd** value to the appropriate $x$ and $y$ coordinates, and delete the **wd** column.\n",
    "\n",
    "`pandas` has the `join` function to take the dataframe we just created to do just this mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>PM2.5</th>\n",
       "      <th>PM10</th>\n",
       "      <th>SO2</th>\n",
       "      <th>NO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>O3</th>\n",
       "      <th>...</th>\n",
       "      <th>station_Guanyuan</th>\n",
       "      <th>station_Gucheng</th>\n",
       "      <th>station_Huairou</th>\n",
       "      <th>station_Nongzhanguan</th>\n",
       "      <th>station_Shunyi</th>\n",
       "      <th>station_Tiantan</th>\n",
       "      <th>station_Wanliu</th>\n",
       "      <th>station_Wanshouxigong</th>\n",
       "      <th>unitX</th>\n",
       "      <th>unitY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>73.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>300.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.123234e-17</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35059</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>19</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35060</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>20</td>\n",
       "      <td>13.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-9.238795e-01</td>\n",
       "      <td>0.382683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35061</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>21</td>\n",
       "      <td>14.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>500.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-7.071068e-01</td>\n",
       "      <td>0.707107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35062</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>22</td>\n",
       "      <td>12.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>400.0</td>\n",
       "      <td>59.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35063</th>\n",
       "      <td>2017</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>13.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>600.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.826834e-01</td>\n",
       "      <td>0.923880</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382168 rows  29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  day  hour  PM2.5  PM10   SO2   NO2     CO    O3  ...  \\\n",
       "0      2013      3    1     0    4.0   4.0   4.0   7.0  300.0  77.0  ...   \n",
       "1      2013      3    1     1    8.0   8.0   4.0   7.0  300.0  77.0  ...   \n",
       "2      2013      3    1     2    7.0   7.0   5.0  10.0  300.0  73.0  ...   \n",
       "3      2013      3    1     3    6.0   6.0  11.0  11.0  300.0  72.0  ...   \n",
       "4      2013      3    1     4    3.0   3.0  12.0  12.0  300.0  72.0  ...   \n",
       "...     ...    ...  ...   ...    ...   ...   ...   ...    ...   ...  ...   \n",
       "35059  2017      2   28    19   11.0  32.0   3.0  24.0  400.0  72.0  ...   \n",
       "35060  2017      2   28    20   13.0  32.0   3.0  41.0  500.0  50.0  ...   \n",
       "35061  2017      2   28    21   14.0  28.0   4.0  38.0  500.0  54.0  ...   \n",
       "35062  2017      2   28    22   12.0  23.0   4.0  30.0  400.0  59.0  ...   \n",
       "35063  2017      2   28    23   13.0  19.0   4.0  38.0  600.0  49.0  ...   \n",
       "\n",
       "       station_Guanyuan  station_Gucheng  station_Huairou  \\\n",
       "0                     0                0                0   \n",
       "1                     0                0                0   \n",
       "2                     0                0                0   \n",
       "3                     0                0                0   \n",
       "4                     0                0                0   \n",
       "...                 ...              ...              ...   \n",
       "35059                 0                0                0   \n",
       "35060                 0                0                0   \n",
       "35061                 0                0                0   \n",
       "35062                 0                0                0   \n",
       "35063                 0                0                0   \n",
       "\n",
       "       station_Nongzhanguan  station_Shunyi  station_Tiantan  station_Wanliu  \\\n",
       "0                         0               0                0               0   \n",
       "1                         0               0                0               0   \n",
       "2                         0               0                0               0   \n",
       "3                         0               0                0               0   \n",
       "4                         0               0                0               0   \n",
       "...                     ...             ...              ...             ...   \n",
       "35059                     0               0                0               0   \n",
       "35060                     0               0                0               0   \n",
       "35061                     0               0                0               0   \n",
       "35062                     0               0                0               0   \n",
       "35063                     0               0                0               0   \n",
       "\n",
       "       station_Wanshouxigong         unitX     unitY  \n",
       "0                          0 -3.826834e-01  0.923880  \n",
       "1                          0  6.123234e-17  1.000000  \n",
       "2                          0 -3.826834e-01  0.923880  \n",
       "3                          0 -7.071068e-01  0.707107  \n",
       "4                          0  6.123234e-17  1.000000  \n",
       "...                      ...           ...       ...  \n",
       "35059                      1 -7.071068e-01  0.707107  \n",
       "35060                      1 -9.238795e-01  0.382683  \n",
       "35061                      1 -7.071068e-01  0.707107  \n",
       "35062                      1 -3.826834e-01  0.923880  \n",
       "35063                      1  3.826834e-01  0.923880  \n",
       "\n",
       "[382168 rows x 29 columns]"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join the direction dataframe with the data, mapping directions to unit circle coordinates\n",
    "data = data.join(directionDf, on = 'wd')\n",
    "\n",
    "# drop the wind direction column\n",
    "data = data.drop(columns = ['wd'])\n",
    "\n",
    "# display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, our data is totally numerical, so we can now fit a regression model!\n",
    "\n",
    "### Train and Test Sets\n",
    "\n",
    "We convert the pollutant columns to a `NumPy` array by applying the `to_numpy()` function to `data`, selecting just the pollutant columns. These are our responses, or $y$ variables in the regression problem, `dataY`. We take the opposite columns to be the data matrix, `dataX`.\n",
    "\n",
    "Then, we use the `train_test_split` function to randomly assign 75\\% of the data to the training set and 25\\% to the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "pollutants = ['PM2.5', 'PM10', 'SO2', 'NO2', 'CO', 'O3']\n",
    "\n",
    "# pollutants are the responses\n",
    "dataY = data[pollutants].to_numpy()\n",
    "\n",
    "# all data except pollutants are predictors\n",
    "dataX = data.drop(columns = pollutants).to_numpy()\n",
    "\n",
    "# split the dataset and labels to 75% training set and 25% test set\n",
    "trainX, testX, trainY, testY = train_test_split(dataX, dataY, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the dimensions of our newly created training and test data to see if it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set dimensions\n",
      "(286626, 23)\n",
      "(286626, 6)\n",
      "\n",
      "Test set dimensions\n",
      "(95542, 23)\n",
      "(95542, 6)\n"
     ]
    }
   ],
   "source": [
    "print('Training set dimensions')\n",
    "print(trainX.shape)\n",
    "print(trainY.shape)\n",
    "\n",
    "print('\\nTest set dimensions')\n",
    "print(testX.shape)\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting Separate Models for Each Response Variable\n",
    "\n",
    "Note that the training set will be quite large (286626 x 286626), which will be difficult with our simple matrix multiplication in the `OrdinaryLeastSquares` class we write, as it is likely to lead to overflows, so let's use the optimized `LinearRegression` class built into the popular `scikit-learn` library and predict each pollutant separately. If you are interested in linear algebra, it is good to know it uses singular value decomposition (SVD)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Modeling PM2.5 pollution =====================\n",
      "The r^2 score is 0.23510056982521432\n",
      "The mean absolute error on the training set is 50.4452441776565\n",
      "The beta values are [ -1.63  -1.33   0.     1.4   -6.03  -1.38   3.78  -4.81  -2.93   1.    -5.7  -10.36   8.58   1.85   3.08 -18.2    6.82   2.71   3.83   0.57   5.83   8.89 -17.08]\n",
      "The mean absolute error on the test set is 50.261496050071955\n",
      "\n",
      "==================== Modeling PM10 pollution =====================\n",
      "The r^2 score is 0.16088318842493265\n",
      "The mean absolute error on the training set is 60.58794671634826\n",
      "The beta values are [ -2.95  -1.23   0.3    1.71  -5.89  -2.52   2.82  -5.94  -1.11   5.08 -11.56 -21.97  11.65   4.52  11.63 -24.54   7.82  -0.21   4.36   4.07   9.15   8.82 -19.96]\n",
      "The mean absolute error on the test set is 60.53901994023608\n",
      "\n",
      "==================== Modeling SO2 pollution =====================\n",
      "The r^2 score is 0.26542293236601733\n",
      "The mean absolute error on the training set is 11.999224649937082\n",
      "The beta values are [-4.67 -1.28 -0.    0.23 -0.78 -0.2  -0.09 -0.51 -1.76  1.68 -0.34 -3.47  2.1   1.71 -1.13 -4.76  2.93 -1.64 -1.3   2.66  1.55  2.28 -5.13]\n",
      "The mean absolute error on the test set is 12.009820053231506\n",
      "\n",
      "==================== Modeling NO2 pollution =====================\n",
      "The r^2 score is 0.38335604870521767\n",
      "The mean absolute error on the training set is 21.00338363899922\n",
      "The beta values are [ -1.64   0.15   0.06   0.63  -2.28  -0.69   0.77  -2.17  -7.41   8.64  -5.72 -23.06   5.69   7.18   1.95 -22.63   9.45  -4.22   4.5   12.55   5.67   3.33  -4.44]\n",
      "The mean absolute error on the test set is 20.970213803565937\n",
      "\n",
      "==================== Modeling CO pollution =====================\n",
      "The r^2 score is 0.3025692914710755\n",
      "The mean absolute error on the training set is 657.5228802970772\n",
      "The beta values are [   1.5    10.29   -2.29   17.04  -95.96  -22.61   42.01  -46.73  -86.39   24.28  -53.2  -293.71  139.64   22.19   64.78 -350.93  134.72    3.43  102.02   45.62  161.16  155.14 -194.05]\n",
      "The mean absolute error on the test set is 658.771412804499\n",
      "\n",
      "==================== Modeling O3 pollution =====================\n",
      "The r^2 score is 0.5271262972430173\n",
      "The mean absolute error on the training set is 29.095340341815362\n",
      "The beta values are [  1.51  -1.8   -0.02   0.94   3.92   0.14  -1.05   1.02   7.37  -1.61  -0.03  11.74  -2.79  -1.62   2.47   7.51  -0.86  -0.94  -4.02  -6.2   -3.65  -1.72 -12.27]\n",
      "The mean absolute error on the test set is 29.162701684609818\n"
     ]
    }
   ],
   "source": [
    "# import the linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate an OLS model\n",
    "model = LinearRegression()\n",
    "\n",
    "for i in range(trainY.shape[1]):\n",
    "    # choose the pollutant\n",
    "    print('\\n==================== Modeling', pollutants[i], 'pollution =====================')\n",
    "\n",
    "    # fit the model to the training data (find the beta parameters)\n",
    "    model.fit(trainX, trainY[:, i])\n",
    "\n",
    "    # return the predicted outputs for the datapoints in the training set\n",
    "    trainPredictions = model.predict(trainX)\n",
    "\n",
    "    # print the coefficient of determination r^2\n",
    "    print('The r^2 score is', model.score(trainX, trainY[:, i]))\n",
    "\n",
    "    # print quality metrics\n",
    "    print('The mean absolute error on the training set is', mean_absolute_error(trainY[:, i], trainPredictions))\n",
    "    \n",
    "    # print the beta values\n",
    "    #print('The beta values are', model.beta)\n",
    "    print('The beta values are', np.round(model.coef_, 2))\n",
    "\n",
    "    # return the predicted outputs for the datapoints in the test set\n",
    "    predictions = model.predict(testX)\n",
    "\n",
    "    # print quality metrics\n",
    "    print('The mean absolute error on the test set is', mean_absolute_error(testY[:, i], predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting a Multivariate Regression Model\n",
    "\n",
    "The `OrdinaryLeastSquares` class we wrote was for multiple regression (multiple inputs) but not multivariate regression (multiple outputs). We will again use the `LinearRegression` class from `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The r^2 score is 0.3124097213393054\n",
      "The mean absolute error on the training set is 138.44233663696912\n",
      "\n",
      "The beta values for predicting PM2.5 are\n",
      " [ -1.63  -1.33   0.     1.4   -6.03  -1.38   3.78  -4.81  -2.93   1.    -5.7  -10.36   8.58   1.85   3.08 -18.2    6.82   2.71   3.83   0.57   5.83   8.89 -17.08]\n",
      "\n",
      "The beta values for predicting PM10 are\n",
      " [ -2.95  -1.23   0.3    1.71  -5.89  -2.52   2.82  -5.94  -1.11   5.08 -11.56 -21.97  11.65   4.52  11.63 -24.54   7.82  -0.21   4.36   4.07   9.15   8.82 -19.96]\n",
      "\n",
      "The beta values for predicting SO2 are\n",
      " [-4.67 -1.28 -0.    0.23 -0.78 -0.2  -0.09 -0.51 -1.76  1.68 -0.34 -3.47  2.1   1.71 -1.13 -4.76  2.93 -1.64 -1.3   2.66  1.55  2.28 -5.13]\n",
      "\n",
      "The beta values for predicting NO2 are\n",
      " [ -1.64   0.15   0.06   0.63  -2.28  -0.69   0.77  -2.17  -7.41   8.64  -5.72 -23.06   5.69   7.18   1.95 -22.63   9.45  -4.22   4.5   12.55   5.67   3.33  -4.44]\n",
      "\n",
      "The beta values for predicting CO are\n",
      " [   1.5    10.29   -2.29   17.04  -95.96  -22.61   42.01  -46.73  -86.39   24.28  -53.2  -293.71  139.64   22.19   64.78 -350.93  134.72    3.43  102.02   45.62  161.16  155.14 -194.05]\n",
      "\n",
      "The beta values for predicting O3 are\n",
      " [  1.51  -1.8   -0.02   0.94   3.92   0.14  -1.05   1.02   7.37  -1.61  -0.03  11.74  -2.79  -1.62   2.47   7.51  -0.86  -0.94  -4.02  -6.2   -3.65  -1.72 -12.27]\n",
      "\n",
      "The mean absolute error on the test set is 138.61911072270158\n"
     ]
    }
   ],
   "source": [
    "# instantiate an OLS model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model to the training data (find the beta parameters)\n",
    "model.fit(trainX, trainY)\n",
    "\n",
    "# return the predicted outputs for the datapoints in the training set\n",
    "trainPredictions = model.predict(trainX)\n",
    "\n",
    "# print the coefficient of determination r^2\n",
    "print('The r^2 score is', model.score(trainX, trainY))\n",
    "\n",
    "# print quality metrics\n",
    "print('The mean absolute error on the training set is', mean_absolute_error(trainY, trainPredictions))\n",
    "\n",
    "# return the predicted outputs for the datapoints in the test set\n",
    "predictions = model.predict(testX)\n",
    "\n",
    "# print the beta values\n",
    "betas = [np.round(row, 2) for row in model.coef_]\n",
    "for i in range(6):\n",
    "    print('\\nThe beta values for predicting', pollutants[i], 'are\\n', betas[i])\n",
    "\n",
    "# print quality metrics\n",
    "print('\\nThe mean absolute error on the test set is', mean_absolute_error(testY, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might note, there is actually no difference between the models in the last two code blocks. The parameters are the same for each pollutant. The only difference is that we have wholistic measures of quality, which are simply averages of the results from above.\n",
    "\n",
    "In a sense, the linear regression models are just stacked on one another. It turns out, this is a common situation in machine learning. In fact, neural networks (deep learning) that solve regression problems can be considered as stacked models. The difference is that neural networks allows the responses variable to *share* parameters--so all response variables would be influenced by all of the parameters in the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (DL)",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
